{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "import boto3\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Code for Taxi Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Bucket Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3_BUCKET = os.getenv('AWS_S3_BUCKET')\n",
    "s3_client = session.client('s3')\n",
    "bucket_name = \"nyc-tlc\"\n",
    "file_key = \"trip data/yellow_tripdata_2023-03.parquet\"\n",
    "local_file = \"data/raw/yellow_tripdata_2023-03.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download file: An error occurred (403) when calling the HeadObject operation: Forbidden\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s3_client.download_file(bucket_name, file_key, local_file)\n",
    "    print(f\"Downloaded {file_key} from {bucket_name} to {local_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, the 2023 yellow taxi data would be available in an AWS S3 bucket, then we could use `pyspark.sql.SparkSession` to read the file from the hosting S3 bucket to another storage efficiently. However, even with AWS credentials, we get either `forbidden` or `Access Denied` errors when attempting to download the data or list objects in the bucket in Python and CLI. This has been a documented issue which is discussed on [awslabs open-data-registry GitHub issues page](https://github.com/awslabs/open-data-registry/issues/1418). Below I attempt another method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting API Requests Directly to NYC Open Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I attempted to construct the dataset using the [NYC OpenData API for the Yellow Taxi 2023 full dataset](https://data.cityofnewyork.us/Transportation/2023-Yellow-Taxi-Trip-Data/4b4i-vvec/about_data). This ended up being way too slow given the size of the data (~38.3 million rows), API request batch size limits (50,000), and throttling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_taxi_data_info(output_file):\n",
    "    \"\"\"Check if an existing Parquet file is present and return metadata.\"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        existing_table = pq.read_table(output_file)\n",
    "        offset = existing_table.num_rows\n",
    "        schema_columns = existing_table.column_names\n",
    "        del existing_table\n",
    "        print(f\"üîÑ Resuming from offset {offset} (Existing rows: {offset})\")\n",
    "    else:\n",
    "        offset = 0\n",
    "        schema_columns = None\n",
    "        print(\"üöÄ Starting new data fetch...\")\n",
    "    return offset, schema_columns\n",
    "\n",
    "\n",
    "def fetch_taxi_data(api_url, offset, batch_size = 50000, order = \"tpep_pickup_datetime ASC\", max_retries = 5, retry_delay = 10):\n",
    "    \"\"\"Fetch taxi data from the API with exponential backoff.\"\"\"\n",
    "    params = {\n",
    "        \"$limit\": batch_size,\n",
    "        \"$offset\": offset,\n",
    "        \"$order\": order or \"tpep_pickup_datetime ASC\",\n",
    "    }\n",
    "\n",
    "    # Attempt the request multiple times with exponential backoff\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = requests.get(api_url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ö†Ô∏è Request failed (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(retry_delay * (2 ** attempt)) # Exponential backoff\n",
    "\n",
    "    print(f\"‚ùå Max retries exceeded. Skipping offset {offset}.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_and_save_data(output_file, batch_data, first_write):\n",
    "    \"\"\"Process and append batch data to the Parquet file.\"\"\"\n",
    "    if not batch_data:\n",
    "        print(\"üèÅ Reached end of dataset. Done fetching!\")\n",
    "        return False\n",
    "    \n",
    "    df = pd.DataFrame(batch_data)  # Convert batch to DataFrame\n",
    "    table = pa.Table.from_pandas(df)  # Convert DataFrame to Apache Arrow Table\n",
    "\n",
    "    # Initialize ParquetWriter on first batch, else append to existing file\n",
    "    if first_write:\n",
    "        parquet_writer = pq.ParquetWriter(output_file, table.schema, compression=\"snappy\")\n",
    "        parquet_writer.write_table(table)\n",
    "        parquet_writer.close()\n",
    "        print(\"üìù First batch written!\")\n",
    "    else:\n",
    "        existing_table = pq.read_table(output_file)\n",
    "        table = table.select(existing_table.column_names)  # Reorder new table to match existing schema\n",
    "        combined_table = pa.concat_tables([existing_table, table])\n",
    "        pq.write_table(combined_table, output_file, compression=\"snappy\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def main(api_url, output_file, batch_size=50000, max_retries=5, retry_delay=10):\n",
    "    \"\"\"Main function to fetch and save NYC Yellow Taxi data.\"\"\"\n",
    "    offset, _ = get_existing_taxi_data_info(output_file)\n",
    "    total_rows_fetched = offset\n",
    "    first_write = not os.path.exists(output_file)\n",
    "\n",
    "    while True:\n",
    "        # Fetch a batch of taxi data\n",
    "        batch_data = fetch_taxi_data(api_url, offset, batch_size, max_retries=max_retries, retry_delay=retry_delay)\n",
    "\n",
    "        if not batch_data:\n",
    "            break  # Stop if no more data is available\n",
    "\n",
    "        # Process and save the batch data \n",
    "        if not process_and_save_data(output_file, batch_data, first_write):\n",
    "            break  # stop if data processing fails\n",
    "\n",
    "        # Increment counters\n",
    "        total_rows_fetched += len(batch_data)\n",
    "        offset += batch_size\n",
    "        first_write = False\n",
    "\n",
    "        print(f\"‚úÖ Fetched {total_rows_fetched} rows so far...\")\n",
    "\n",
    "        # Short delay to avoid API rate limits\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(f\"üìÅ Dataset successfully saved to {output_file} with {total_rows_fetched} rows.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the full dataset\n",
    "from src.config import RAW_DATA_DIR\n",
    "\n",
    "params = {\n",
    "    \"api_url\": \"https://data.cityofnewyork.us/resource/4b4i-vvec.json\",\n",
    "    \"output_file\": os.path.join(RAW_DATA_DIR, \"yellow_taxi_2023_full.parquet\"),\n",
    "    \"batch_size\": 50000,  # Max records per request\n",
    "    \"max_retries\": 5,  # Max retries per request\n",
    "    \"retry_delay\": 10,  # Base delay between request retries (exponential backoff)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Resuming from offset 3250000 (Existing rows: 3250000)\n",
      "‚ö†Ô∏è Request failed (attempt 1/5): HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Max retries exceeded with url: /resource/4b4i-vvec.json?%24limit=50000&%24offset=3250000&%24order=tpep_pickup_datetime+ASC (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001244D8DFD40>: Failed to resolve 'data.cityofnewyork.us' ([Errno 11002] getaddrinfo failed)\"))\n",
      "‚ö†Ô∏è Request failed (attempt 2/5): HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Max retries exceeded with url: /resource/4b4i-vvec.json?%24limit=50000&%24offset=3250000&%24order=tpep_pickup_datetime+ASC (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001244D9FEED0>: Failed to resolve 'data.cityofnewyork.us' ([Errno 11001] getaddrinfo failed)\"))\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3300000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3350000 rows so far...\n",
      "‚ö†Ô∏è Request failed (attempt 1/5): HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Max retries exceeded with url: /resource/4b4i-vvec.json?%24limit=50000&%24offset=3350000&%24order=tpep_pickup_datetime+ASC (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000012456798800>: Failed to resolve 'data.cityofnewyork.us' ([Errno 11002] getaddrinfo failed)\"))\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3400000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3450000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3500000 rows so far...\n",
      "‚ö†Ô∏è Request failed (attempt 1/5): HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Max retries exceeded with url: /resource/4b4i-vvec.json?%24limit=50000&%24offset=3500000&%24order=tpep_pickup_datetime+ASC (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000124567A0C50>: Failed to resolve 'data.cityofnewyork.us' ([Errno 11001] getaddrinfo failed)\"))\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3550000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3600000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3650000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3700000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3750000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3800000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3850000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3900000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 3950000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 4000000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 4050000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 4100000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 4150000 rows so far...\n",
      "‚ö†Ô∏è Request failed (attempt 1/5): HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Max retries exceeded with url: /resource/4b4i-vvec.json?%24limit=50000&%24offset=4150000&%24order=tpep_pickup_datetime+ASC (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001245A6E10D0>: Failed to resolve 'data.cityofnewyork.us' ([Errno 11001] getaddrinfo failed)\"))\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 4200000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 4250000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 4300000 rows so far...\n",
      "üìù Appended 50000 new rows.\n",
      "‚úÖ Fetched 4350000 rows so far...\n",
      "‚ö†Ô∏è Request failed (attempt 1/5): HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Max retries exceeded with url: /resource/4b4i-vvec.json?%24limit=50000&%24offset=4350000&%24order=tpep_pickup_datetime+ASC (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000012457B268A0>: Failed to resolve 'data.cityofnewyork.us' ([Errno 11001] getaddrinfo failed)\"))\n",
      "‚ö†Ô∏è Request failed (attempt 2/5): HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Max retries exceeded with url: /resource/4b4i-vvec.json?%24limit=50000&%24offset=4350000&%24order=tpep_pickup_datetime+ASC (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000012457B278F0>: Failed to resolve 'data.cityofnewyork.us' ([Errno 11001] getaddrinfo failed)\"))\n",
      "‚ö†Ô∏è Request failed (attempt 3/5): HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Max retries exceeded with url: /resource/4b4i-vvec.json?%24limit=50000&%24offset=4350000&%24order=tpep_pickup_datetime+ASC (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000012457B275F0>: Failed to resolve 'data.cityofnewyork.us' ([Errno 11001] getaddrinfo failed)\"))\n",
      "‚ö†Ô∏è Request failed (attempt 4/5): HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Max retries exceeded with url: /resource/4b4i-vvec.json?%24limit=50000&%24offset=4350000&%24order=tpep_pickup_datetime+ASC (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000012457B27530>: Failed to resolve 'data.cityofnewyork.us' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    }
   ],
   "source": [
    "main(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Shape: 50000 rows, 19 columns\n",
      "      vendorid     tpep_pickup_datetime    tpep_dropoff_datetime  \\\n",
      "33553        2  2023-01-08T15:45:11.000  2023-01-08T15:50:41.000   \n",
      "9427         2  2023-01-08T11:13:26.000  2023-01-08T11:18:24.000   \n",
      "199          2  2023-01-08T07:54:52.000  2023-01-08T08:04:38.000   \n",
      "12447        2  2023-01-08T11:53:13.000  2023-01-08T12:00:43.000   \n",
      "39489        1  2023-01-08T16:46:34.000  2023-01-08T16:53:23.000   \n",
      "42724        1  2023-01-08T17:21:13.000  2023-01-08T17:37:58.000   \n",
      "10822        2  2023-01-08T11:31:58.000  2023-01-08T11:46:48.000   \n",
      "49498        2  2023-01-08T18:40:50.000  2023-01-08T18:49:37.000   \n",
      "4144         1  2023-01-08T09:51:50.000  2023-01-08T09:57:02.000   \n",
      "36958        1  2023-01-08T16:20:08.000  2023-01-08T16:41:31.000   \n",
      "\n",
      "      trip_distance pulocationid dolocationid payment_type fare_amount extra  \\\n",
      "33553          1.14          161          170            1         7.9   0.0   \n",
      "9427           1.59          141          263            1         8.6   0.0   \n",
      "199            2.46          141          238            1        13.5   0.0   \n",
      "12447          0.76          237          161            1         8.6   0.0   \n",
      "39489           1.0          237          140            1         7.9   2.5   \n",
      "42724           2.5          211          233            1        15.6   2.5   \n",
      "10822          3.71          142           74            1        18.4   0.0   \n",
      "49498          2.47          233          262            2        12.8   0.0   \n",
      "4144            0.8           43          141            0         7.2   0.0   \n",
      "36958           1.7          161          186            1        19.1   2.5   \n",
      "\n",
      "      mta_tax tip_amount tolls_amount improvement_surcharge total_amount  \\\n",
      "33553     0.5       3.57          0.0                   1.0        15.47   \n",
      "9427      0.5        1.0          0.0                   1.0         13.6   \n",
      "199       0.5       4.38          0.0                   1.0        21.88   \n",
      "12447     0.5       2.52          0.0                   1.0        15.12   \n",
      "39489     0.5        2.4          0.0                   1.0         14.3   \n",
      "42724     0.5        4.9          0.0                   1.0         24.5   \n",
      "10822     0.5       4.48          0.0                   1.0        26.88   \n",
      "49498     0.5        0.0          0.0                   1.0         16.8   \n",
      "4144      0.5       2.24          0.0                   1.0        13.44   \n",
      "36958     0.5        4.6          0.0                   1.0         27.7   \n",
      "\n",
      "      passenger_count ratecodeid store_and_fwd_flag congestion_surcharge  \\\n",
      "33553             1.0        1.0                  N                  2.5   \n",
      "9427              1.0        1.0                  N                  2.5   \n",
      "199               1.0        1.0                  N                  2.5   \n",
      "12447             1.0        1.0                  N                  2.5   \n",
      "39489             2.0        1.0                  N                  2.5   \n",
      "42724             1.0        1.0                  N                  2.5   \n",
      "10822             3.0        1.0                  N                  2.5   \n",
      "49498             1.0        1.0                  N                  2.5   \n",
      "4144             None       None               None                 None   \n",
      "36958             1.0        1.0                  N                  2.5   \n",
      "\n",
      "      airport_fee  \n",
      "33553         0.0  \n",
      "9427          0.0  \n",
      "199           0.0  \n",
      "12447         0.0  \n",
      "39489         0.0  \n",
      "42724         0.0  \n",
      "10822         0.0  \n",
      "49498         0.0  \n",
      "4144         None  \n",
      "36958         0.0  \n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Parquet file\n",
    "table = pq.read_table(params[\"output_file\"])\n",
    "\n",
    "# Get the number of rows and columns\n",
    "num_rows, num_columns = table.num_rows, len(table.schema)\n",
    "\n",
    "# Convert to Pandas DataFrame for sampling\n",
    "df_sample = table.to_pandas().sample(n=10, random_state=42)  # Get 10 random rows\n",
    "\n",
    "# Print dataset shape\n",
    "print(f\"üìä Dataset Shape: {num_rows} rows, {num_columns} columns\")\n",
    "\n",
    "# Display sample rows\n",
    "print(df_sample)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc_taxi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
